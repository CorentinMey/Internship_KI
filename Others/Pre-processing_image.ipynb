{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52aec6c0-9814-4df9-804d-e8b510ecfac1",
   "metadata": {},
   "source": [
    "Python module to pre-process image --> histogram equalization algorithm and Gaussian filtering used on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cfc8f4c-56e1-4cb0-a0ba-1797a57e924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877d819-6136-4fe2-b633-ba9494113b35",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7728b95-422d-48e9-a3fb-6451ca2b6117",
   "metadata": {},
   "source": [
    "A high resolution image (jpg) (breast cancer biopsie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "281f16ce-5398-4560-801d-5fe318e2e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image 34C\n",
    "path=\"/disk2/user/cda/SpatialTranscriptomics/raw-data/High-resolution_tissue_images/V10F03-034/210223_BC_S7_V10F03-034_RJ.C1-Spot000001.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbc6634-b6de-409a-9b05-44c5b8b6f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image 34D\n",
    "path=\"/disk2/user/cda/SpatialTranscriptomics/raw-data/High-resolution_tissue_images/V10F03-034/210223_BC_S7_V10F03-034_RJ.D1-Spot000001.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28709da1-0e9e-49a3-9ab1-dc08266c5bbe",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b30a8a-549b-4789-ae4c-d0b14c09b331",
   "metadata": {},
   "source": [
    "A pre-processed image (improved contrast, reduced background noise and unwanted details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7cec49-5d90-4cf0-ab6b-b56176d96101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'034_RJ.C1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the name of the output if you want to create a new image\n",
    "image_info = path.split(\"/\")[-1]\n",
    "image_info = image_info.split(\"-\")[1]\n",
    "output_path=\"/disk2/user/cormey/outputs/pre-processed_images/{}.gray2.jpg\".format(image_info)\n",
    "image_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2591f4f1-2a15-45bc-b615-6c8b2512b062",
   "metadata": {},
   "source": [
    "# Open the Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71bcf1d9-d1ca-4eb8-80b6-145e311c5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "# Charge the image with PIL\n",
    "image = Image.open(path)\n",
    "\n",
    "# Get the dimensions of the image\n",
    "width, height = image.size\n",
    "\n",
    "# Divide the image in 4 parts because the image is too big to be open \n",
    "# The 4 parts will be processed seperately before getting regathered\n",
    "top_left = image.crop((0, 0, width // 2, height // 2))\n",
    "top_right = image.crop((width // 2, 0, width, height // 2))\n",
    "bottom_left = image.crop((0, height // 2, width // 2, height))\n",
    "bottom_right = image.crop((width // 2, height // 2, width, height))\n",
    "\n",
    "list_image=[]\n",
    "list_image.append(top_left)\n",
    "list_image.append(top_right)\n",
    "list_image.append(bottom_left)\n",
    "list_image.append(bottom_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094e954a-0966-4350-8326-2c11a52599bf",
   "metadata": {},
   "source": [
    "# Method 1 (insufficient results, not used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23688dd5-3308-4cf9-a012-340b12a78397",
   "metadata": {},
   "source": [
    "## Histogram equalization algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92ec69-c2a3-4a65-818b-751a181ad217",
   "metadata": {},
   "source": [
    "Improve the contrast of an image by redistributing the intensity of each of the image pixels --> dark areas become darker and bright areas become brighter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c85d63e-5623-4a2a-8340-c9859c1d0918",
   "metadata": {},
   "source": [
    "### By using a gray scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5830b8-b4f0-4ec7-a4e0-fe190d787a48",
   "metadata": {},
   "source": [
    "Easier to realize, manipulate only 1 value of intensity per pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39bad8ca-bd6f-411d-9809-1d9fafad0a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the color image in a gray scale image\n",
    "list_image2=[]\n",
    "for image in list_image:\n",
    "    stock=cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "    stock = cv2.cvtColor(stock, cv2.COLOR_BGR2GRAY)\n",
    "    list_image2.append(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e6948c2-c6be-437e-b62a-fa8eccdf9dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform the Histogram equalization algorithm\n",
    "list_equalized_image = []\n",
    "for image in list_image2:\n",
    "    stock=cv2.equalizeHist(image)\n",
    "    list_equalized_image.append(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58cafdf-1752-4139-bb57-4f2d934f0d82",
   "metadata": {},
   "source": [
    "### By conserving colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34f205-26f2-4e73-a867-2d2ce15d96f1",
   "metadata": {},
   "source": [
    "Handle the equalization on differents types of color channels. The goal is to seperate color channels of the image, to only use the equalization on the brightness channel, and then to remerge the channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec45f26a-b1d2-48e0-98fc-7339c8855459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalization_colors(image):\n",
    "\n",
    "    # Convert in space of color YCbCr\n",
    "    ycbcr_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    # Separate the color channels\n",
    "    y, cr, cb = cv2.split(ycbcr_image)\n",
    "    \n",
    "    # Equalization on channel Y\n",
    "    y_eq = cv2.equalizeHist(y)\n",
    "    \n",
    "    # Remerge channels\n",
    "    ycbcr_eq_image = cv2.merge((y_eq, cr, cb))\n",
    "    \n",
    "    # Convert from YCbCr to RGB\n",
    "    equalized_image = cv2.cvtColor(ycbcr_eq_image, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    return equalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4853023c-2331-44ba-9f82-d420d8d04c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_equalized_image = []\n",
    "for image in list_image:\n",
    "    stock=equalization_colors(image)\n",
    "    list_equalized_image.append(stock)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0f7fe3-e0bd-430b-a625-3a26901a132b",
   "metadata": {},
   "source": [
    "## Gaussian filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb6fd3-c622-4c4c-bfa2-ed1b376ab846",
   "metadata": {},
   "source": [
    "Reduce background noise and unwanted details. Use the gaussian function to calculate the weight of each pixel. Blur the image with a Gaussien kernel, to average noisy pixel with their neighbor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d47ba7-db18-4611-bf56-c55fb61a4f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_processed_image=[]\n",
    "kernel_size = (5, 5)  # Size of the nucleus (must be an odd number)\n",
    "sigma = 0  # Leave at 0 so that OpenCV automatically calculates based on kernel size\n",
    "for image in list_equalized_image:\n",
    "    processed_image = cv2.GaussianBlur(image, kernel_size, sigma)\n",
    "    list_processed_image.append(processed_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9bcf0a-f979-4751-b7a1-2b8265c378f0",
   "metadata": {},
   "source": [
    "# Method 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316405f2-2e53-4a7e-80c9-364248d82c5e",
   "metadata": {},
   "source": [
    "Use of CLAHE, a bilateral filter, and Canny edge detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3de8e-edc8-431e-8a79-0cccff4ee906",
   "metadata": {},
   "source": [
    "-CLAHE : (Contrast Limited Adaptive Histogram Equalization), which can limit noise amplification while improving contrast locally.\n",
    "-Bilateral filter : reduces noise while preserving edges.\n",
    "-Canny : edge detection technique, to highlight cell edges."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca10324-4224-4a30-a9c6-6454ef18dc58",
   "metadata": {},
   "source": [
    "By using a gray scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f573a992-e106-4b1c-9d53-a543aeebcb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_processed_image=[]\n",
    "for image in list_image:\n",
    "\n",
    "    #Tranform in numpy array to perform clahe\n",
    "    image=np.array(image)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    clahe_image = clahe.apply(image)\n",
    "    \n",
    "    # Apply bilateral filtering\n",
    "    bilateral_filtered_image = cv2.bilateralFilter(clahe_image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    list_processed_image.append(bilateral_filtered_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5bc42b-9f08-4270-9147-0c0974afe705",
   "metadata": {},
   "source": [
    "By conserving colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb70e72-3f1b-4ee1-b34a-2e114f6f0c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clahe_equalizer(image):\n",
    "    # Convert in space of color YCbCr\n",
    "    ycbcr_image = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2YCrCb)\n",
    "    \n",
    "    # Separate the color channels\n",
    "    y, cr, cb = cv2.split(ycbcr_image)\n",
    "    \n",
    "    # Equalization on channel Y\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    y_eq = clahe.apply(y)\n",
    "    \n",
    "    \n",
    "    # Remerge channels\n",
    "    ycbcr_eq_image = cv2.merge((y_eq, cr, cb))\n",
    "    \n",
    "    # Convert from YCbCr to RGB\n",
    "    equalized_image = cv2.cvtColor(ycbcr_eq_image, cv2.COLOR_YCrCb2BGR)\n",
    "\n",
    "    return equalized_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee9c0865-5e58-452c-bd0a-c586ab003c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_processed_image=[]\n",
    "for image in list_image:\n",
    "\n",
    "    # Apply CLAHE\n",
    "    clahe_image=clahe_equalizer(image)\n",
    "    \n",
    "    # Apply bilateral filtering\n",
    "    bilateral_filtered_image = cv2.bilateralFilter(clahe_image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "    list_processed_image.append(bilateral_filtered_image)\n",
    "    \n",
    "    # Edges detection with Canny\n",
    "    #edges = cv2.Canny(bilateral_filtered_image, threshold1=100, threshold2=200) #Not used for now, maybe add edge to the figure can be useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931f8056-5856-4474-ab38-fbca1ed9edad",
   "metadata": {},
   "source": [
    "#  Regathering images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9421177e-9863-4018-950a-9216d69e5ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_row = np.hstack((list_processed_image[0], list_processed_image[1])) #gathers horizontally\n",
    "bottom_row = np.hstack((list_processed_image[2], list_processed_image[3])) #gathers horizontally\n",
    "combined_image = np.vstack((top_row, bottom_row)) #gathers vertically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a8d66-91fe-4f30-8b18-1d9bf8271464",
   "metadata": {},
   "source": [
    "# Save the processed image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9dd97fa-b928-4b63-9aab-6bbdd0d81ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(output_path, combined_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06d3b2-2a52-40b9-bcee-56634fbf84ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094e4e8-f224-4068-8829-d7d3bbbf9ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
